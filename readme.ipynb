{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div id=\"table-of-contents\" role=\"doc-toc\">\n<h2>Table of Contents</h2>\n<div id=\"text-table-of-contents\" role=\"doc-toc\">\n<ul>\n<li><a href=\"#org80d305d\">1. Introduction</a>\n<ul>\n<li><a href=\"#orgc93cd38\">1.1. Task Definition</a></li>\n</ul>\n</li>\n<li><a href=\"#orgf55ae53\">2. Libraries</a>\n<ul>\n<li><a href=\"#org0c6aec6\">2.1. Import Modules</a></li>\n</ul>\n</li>\n<li><a href=\"#org89999d7\">3. Algorithm &amp; Training</a>\n<ul>\n<li><a href=\"#org3815c1b\">3.1. Environment</a>\n<ul>\n<li><a href=\"#orgf6d2c7f\">3.1.1. Custom Environment</a></li>\n</ul>\n</li>\n<li><a href=\"#orga51b802\">3.2. Model &amp; Training</a></li>\n<li><a href=\"#org86d2305\">3.3. Observing the performance in real-time</a></li>\n<li><a href=\"#org79f218d\">3.4. Creating the Video</a></li>\n</ul>\n</li>\n<li><a href=\"#org60f597c\">4. Execution</a></li>\n</ul>\n</div>\n</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"org80d305d\"></a>\n\n# Introduction\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In my current field of Computer Vision, particularly in Object Detection,\noperating within a simulated environment poses certain challenges. These\nchallenges, including but not limited to utilizing pre-labeled datasets for\ntraining ML models or setting up an environment with adequate objects for the\nrobot to detect the goal accurately can be notably demanding. As a result, I\nhave put together this document to outline an unsupervised robotic manipulation\ntask.\n\nAs the given challenge revolves more around the ability of the candidate to\nwork with a robotic simulation platform, I decided to use an RL-base algorithm\ncalled Deep Deterministic Policy Gradient (DDPG) to train an agent to control\nthe system's actuators towards a goal.\n\nThis documentation describes a Python script that trains an agent using the\nalgorithm in an environment provided by OpenAI Gym. The script employs the\nStable Baselines3 library for reinforcement learning and training tasks.\n\nThis document outlines and explains the required step to train an agent to\ncontrol the inverted pendulum in an upright position using a simulated\nenvironment provided by the OpenAI Gym module. The agent uses the\n`stable_baseline3` library to learn the task, that is creating a policy that given\na state is able to lead the system towards an optimal point.\n\nI encourage you to explore my other repositories to observe my capabilities in\ntraining Deep Neural Networks for a wide range of Computer Vision tasks,\nincluding Image Classification and Object Detection.\n\n-   <a href=\"https://github.com/soheilred/FogGuard\">Object Detection Under Adverse Weather Condition</a>\n-   <a href=\"https://github.com/soheilred/InCoP\">How to Efficiently Search for Sparse Networks?</a>\n\nIn addition, you can find my repository corresponding to the Human-Computer\nInteractions course project in <a href=\"https://github.com/soheilred/hri/tree/main/src\">here</a>, where I explored various environment and RL\nmethods to find the best algorithm for our project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"orgc93cd38\"></a>\n\n## Task Definition\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The chosen task for training the robot involves controlling the Inverted\nPendulum in the InvertedPendulum-v5 environment. The task of balancing an\ninverted pendulum is a classic problem in control theory and robotics. It\ninvolves keeping a pendulum in an upright position by applying appropriate\ncontrol actions. This task is relevant to robotics as it represents a\nfundamental problem in designing stabilizing control algorithms for various\nrobotic systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"orgf55ae53\"></a>\n\n# Libraries\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   gym: OpenAI Gym provides a wide range of environments for reinforcement learning.\n-   imageio: A library for reading and writing image data to produce the video.\n-   stable<sub>baselines3</sub>: A reinforcement learning library that implements various algorithms and tools for training agents.\n-   stable<sub>baselines3.common</sub>: Contains common functionalities for training agents, such as noise generation and environment checking.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"org0c6aec6\"></a>\n\n## Import Modules\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's import all the required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 1,
      "source": [
        "import gymnasium as gym\nimport numpy as np\nimport imageio\n\nfrom stable_baselines3 import HerReplayBuffer, SAC, DDPG, TD3\nfrom stable_baselines3.common.noise import NormalActionNoise\nfrom stable_baselines3.common.env_checker import check_env\nfrom stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": ""
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"org89999d7\"></a>\n\n# Algorithm & Training\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The main() function is the entry point of the script and contains the training\n    logic for the robot.\n-   The DDPG model is trained for a set number of steps, saved, and then evaluated.\n-   The script also includes functionality to record and save a video of the\n    trained agent's performance in the environment utilizing the VecVideoRecorder\n    to capture the video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"org3815c1b\"></a>\n\n## Environment\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 1,
      "source": [
        "ENV = \"InvertedPendulum-v5\"\nenv = gym.make(ENV, render_mode=\"rgb_array\")\n\n# wrap the environment\n# env = CustomWrapper(env)\n# check_env(env)\n\n# The noise objects for DDPG\nn_actions = env.action_space.shape[-1]\naction_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": ""
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"orgf6d2c7f\"></a>\n\n### Custom Environment\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are interested into more refined and customized environments, you can use\nthe `CustomWrapper` object. I personally did not use it in my training. However,\nthe lines are left in the code commented so that a curious user is able to use it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 1,
      "source": [
        "class CustomWrapper(gym.Wrapper):\n    \"\"\"\n    :param env: (gym.Env) Gym environment that will be wrapped\n    \"\"\"\n    def __init__(self, env, max_steps=100):\n    # Call the parent constructor, so we can access self.env later\n        super(CustomWrapper, self).__init__(env)\n        self.max_steps = max_steps\n        self.current_step = 0\n\n    def reset(self):\n        obs = self.env.reset()\n        self.current_step = 0\n        obs['desired_goal'] = np.array([1.3, .7, .5], dtype=float) # + 0.001 * np.ones(3) * self.current_step\n        return obs\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        return obs, reward, done, info\n\n    def step_to_goal(self, action):\n        self.current_step += 1\n        T = 50\n        obs, reward, done, info = self.env.step(action)\n        # read the human arm (LRSP) pose based on time\n        obs['desired_goal'] += 0.1 * np.array([np.cos(3 / T * self.current_step),\n                                                 np.sin(3 / T * self.current_step), 0])\n        if self.current_step >= self.max_steps:\n            done = True\n            info['time_limit_reached'] = True\n        return obs, reward, done, info\n\n"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": ""
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"orga51b802\"></a>\n\n## Model & Training\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following block first creates a DDPG object using the defined environment\nand the observation noise, and calls the `learn` function that trains the model to\nbe able to obtain the optimal policy. At the end, we `save` the model in the\ncurrent directory for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 1,
      "source": [
        "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\ntrain_epoch = int(2e5) \nmodel.learn(train_epoch)\nmodel.save(f\"ddpg_{ENV}\")"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": ""
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please note that the chosen `train_epoch` value has direct impact on the\nperformance of the policy, as we are giving more time to the model to learn the\ndynamic of the system. I was able to obtain a somewhat satisfactory result using\n`200000` steps.\n\nAlso, the use of DDPG as the learning algorithm was a design choice and not a\nnecessity for this project. Due to the simplicity of the project, it can be\nsolve using lighter and less involved techniques. We can choose a better\nalgorithm that is more aligned with the requirement of our specific project by\nlooking at stable<sub>baselines</sub>  module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"org86d2305\"></a>\n\n## Observing the performance in real-time\n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 1,
      "source": [
        "obs, _ = env.reset()\n\n# Evaluate the agent\nepisode_reward = 0\nfor _ in range(10):\n    # import ipdb; ipdb.set_trace()\n    action, _states = model.predict(obs)\n    obs, reward, done, info, _ = env.step(action)\n    env.render()"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": ""
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"org79f218d\"></a>\n\n## Creating the Video\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the `VecEnv` object of `stable_baseline3` to parallelize the processes\nand simulate and record a video at the same time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 1,
      "source": [
        "video_folder = 'videos/'\nvideo_length = 100\n\nvec_env = DummyVecEnv([lambda: gym.make(ENV, render_mode=\"rgb_array\")])\n\nobs = vec_env.reset()\n# # Record the video starting at the first step\nvec_env = VecVideoRecorder(vec_env, video_folder,\n                        record_video_trigger=lambda x: x == 0,\n                        video_length=video_length,\n                        name_prefix=f\"ddpg-{ENV}\")\nvec_env.reset()\n\nfor _ in range(video_length + 1):\n    action = model.predict(obs[0])\n    obs, _, _, _ = vec_env.step(action)\n\n# Save the video\nenv.close()"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": ""
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "il": null
      },
      "source": [
        "\n<a id=\"org60f597c\"></a>\n\n# Execution\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run the script:\n\n1.  Ensure that all required libraries are installed.\n2.  Execute the script by running the\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": 1,
      "source": [
        "python ddpg-video.py"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": ""
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "il": null
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
