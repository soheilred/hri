{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4991f75-eb9b-44f4-9626-0b32a4a0d285",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div id=\"table-of-contents\" role=\"doc-toc\">\n",
    "<h2>Table of Contents</h2>\n",
    "<div id=\"text-table-of-contents\" role=\"doc-toc\">\n",
    "<ul>\n",
    "<li><a href=\"#org86c57b1\">1. Introduction</a>\n",
    "<ul>\n",
    "<li><a href=\"#orgfe11cf0\">1.1. Task Definition</a></li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><a href=\"#org966afb4\">2. Libraries</a>\n",
    "<ul>\n",
    "<li><a href=\"#org56c88a6\">2.1. Import Modules</a></li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><a href=\"#orga0b7ec0\">3. Algorithm &amp; Training</a>\n",
    "<ul>\n",
    "<li><a href=\"#orgd238566\">3.1. Environment</a>\n",
    "<ul>\n",
    "<li><a href=\"#org6c0dea0\">3.1.1. Custom Environment</a></li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><a href=\"#orgab3492f\">3.2. Model &amp; Training</a></li>\n",
    "<li><a href=\"#org1778405\">3.3. Observing the performance in real-time</a></li>\n",
    "<li><a href=\"#org70608a1\">3.4. Creating the Video</a></li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><a href=\"#org1d48a23\">4. Execution</a></li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d8deb-2628-4035-83c4-d84d0b23a43f",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"org86c57b1\"></a>\n",
    "\n",
    "# Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9039227-323d-4bbb-b3dc-bb8ca763e8de",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In my current field of Computer Vision, particularly in Object Detection,\n",
    "operating within a simulated environment poses certain challenges. These\n",
    "challenges, including but not limited to utilizing pre-labeled datasets for\n",
    "training ML models or setting up an environment with adequate objects for the\n",
    "robot to detect the goal accurately can be notably demanding. As a result, I\n",
    "have put together this document to outline an unsupervised robotic manipulation\n",
    "task.\n",
    "\n",
    "As the given challenge revolves more around the ability of the candidate to\n",
    "work with a robotic simulation platform, I decided to use an RL-base algorithm\n",
    "called Deep Deterministic Policy Gradient (DDPG) to train an agent to control\n",
    "the system's actuators towards a goal.\n",
    "\n",
    "This documentation describes a Python script that trains an agent using the\n",
    "algorithm in an environment provided by OpenAI Gym. The script employs the\n",
    "Stable Baselines3 library for reinforcement learning and training tasks.\n",
    "\n",
    "This document outlines and explains the required step to train an agent to\n",
    "control the inverted pendulum in an upright position using a simulated\n",
    "environment provided by the OpenAI Gym module. The agent uses the\n",
    "`stable_baseline3` library to learn the task, that is creating a policy that given\n",
    "a state is able to lead the system towards an optimal point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261efa2-9e24-4634-bdb7-a94930eca9d6",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"orgfe11cf0\"></a>\n",
    "\n",
    "## Task Definition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9113ad3-5bae-42e0-9a86-244418a58fb3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The chosen task for training the robot involves controlling the Inverted\n",
    "Pendulum in the InvertedPendulum-v5 environment. The task of balancing an\n",
    "inverted pendulum is a classic problem in control theory and robotics. It\n",
    "involves keeping a pendulum in an upright position by applying appropriate\n",
    "control actions. This task is relevant to robotics as it represents a\n",
    "fundamental problem in designing stabilizing control algorithms for various\n",
    "robotic systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1085de-c111-4e95-9356-e9ddd33d5dea",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"org966afb4\"></a>\n",
    "\n",
    "# Libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139cb99-0d32-440f-83c4-c1961e2662c6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-   gym: OpenAI Gym provides a wide range of environments for reinforcement learning.\n",
    "-   imageio: A library for reading and writing image data to produce the video.\n",
    "-   stable<sub>baselines3</sub>: A reinforcement learning library that implements various algorithms and tools for training agents.\n",
    "-   stable<sub>baselines3.common</sub>: Contains common functionalities for training agents, such as noise generation and environment checking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e0250-719d-45a4-99d9-4231826e81ba",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"org56c88a6\"></a>\n",
    "\n",
    "## Import Modules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0030e1-4897-45df-b3d9-c5c50d294bce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, let's import all the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9179d9-b2a3-4ff2-8ed5-7f4340448935",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "from stable_baselines3 import HerReplayBuffer, SAC, DDPG, TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e400ac2a-8061-4cd8-8098-7a6c9446dad2",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"orga0b7ec0\"></a>\n",
    "\n",
    "# Algorithm & Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466256f1-aa8d-4b9e-a101-fb03ef4f557d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-   The main() function is the entry point of the script and contains the training\n",
    "    logic for the robot.\n",
    "-   The DDPG model is trained for a set number of steps, saved, and then evaluated.\n",
    "-   The script also includes functionality to record and save a video of the\n",
    "    trained agent's performance in the environment utilizing the VecVideoRecorder\n",
    "    to capture the video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afd44f9-2441-4022-a1e2-6d2c3108bf09",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"orgd238566\"></a>\n",
    "\n",
    "## Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2baf55fa-a749-4927-b31c-bb8506211cb4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ENV = \"InvertedPendulum-v5\"\n",
    "env = gym.make(ENV, render_mode=\"rgb_array\")\n",
    "\n",
    "# wrap the environment\n",
    "# env = CustomWrapper(env)\n",
    "# check_env(env)\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd42d16-7483-4f8d-b808-8d3ed98de5b8",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"org6c0dea0\"></a>\n",
    "\n",
    "### Custom Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c247326-4799-4637-8be2-a54f8bc9d470",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you are interested into more refined and customized environments, you can use\n",
    "the `CustomWrapper` object. I personally did not use it in my training. However,\n",
    "the lines are left in the code commented so that a curious user is able to use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9129333e-6345-4a13-a1e3-7df15f146e04",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "class CustomWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    :param env: (gym.Env) Gym environment that will be wrapped\n",
    "    \"\"\"\n",
    "    def __init__(self, env, max_steps=100):\n",
    "    # Call the parent constructor, so we can access self.env later\n",
    "        super(CustomWrapper, self).__init__(env)\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        self.current_step = 0\n",
    "        obs['desired_goal'] = np.array([1.3, .7, .5], dtype=float) # + 0.001 * np.ones(3) * self.current_step\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def step_to_goal(self, action):\n",
    "        self.current_step += 1\n",
    "        T = 50\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        # read the human arm (LRSP) pose based on time\n",
    "        obs['desired_goal'] += 0.1 * np.array([np.cos(3 / T * self.current_step),\n",
    "                                                 np.sin(3 / T * self.current_step), 0])\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "            info['time_limit_reached'] = True\n",
    "        return obs, reward, done, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39d65e-34bd-44f9-a381-63ef42954628",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"orgab3492f\"></a>\n",
    "\n",
    "## Model & Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581cacb-5ecc-426a-b2e9-77a8b9b01dad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following block first creates a DDPG object using the defined environment\n",
    "and the observation noise, and calls the `learn` function that trains the model to\n",
    "be able to obtain the optimal policy. At the end, we `save` the model in the\n",
    "current directory for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ba4c6a-a6ff-4daf-a855-0c7c3216b724",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model = DDPG(\"MlpPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "train_epoch = int(2e5) \n",
    "model.learn(train_epoch)\n",
    "model.save(f\"ddpg_{ENV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784c16c-c9b9-479e-bfa8-ff62b43ddee4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Please note that the chosen `train_epoch` value has direct impact on the\n",
    "performance of the policy, as we are giving more time to the model to learn the\n",
    "dynamic of the system. I was able to obtain a somewhat satisfactory result using\n",
    "`200000` steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a827f-7d9b-4452-9363-e17b66bdca14",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"org1778405\"></a>\n",
    "\n",
    "## Observing the performance in real-time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f975992-bbe4-42b7-80df-98302179cf72",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "# Evaluate the agent\n",
    "episode_reward = 0\n",
    "for _ in range(10):\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85212e38-6932-45ea-ad3e-99eca8ffb424",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"org70608a1\"></a>\n",
    "\n",
    "## Creating the Video\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b9cf1c-3b64-4299-9ce4-39fe36c312f9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can use the `VecEnv` object of `stable_baseline3` to parallelize the processes\n",
    "and simulate and record a video at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7766ae02-095a-4e76-88fc-c2fb9ec29dcf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "video_folder = 'videos/'\n",
    "video_length = 100\n",
    "\n",
    "vec_env = DummyVecEnv([lambda: gym.make(ENV, render_mode=\"rgb_array\")])\n",
    "\n",
    "obs = vec_env.reset()\n",
    "# # Record the video starting at the first step\n",
    "vec_env = VecVideoRecorder(vec_env, video_folder,\n",
    "                        record_video_trigger=lambda x: x == 0,\n",
    "                        video_length=video_length,\n",
    "                        name_prefix=f\"ddpg-{ENV}\")\n",
    "vec_env.reset()\n",
    "\n",
    "for _ in range(video_length + 1):\n",
    "    action = model.predict(obs[0])\n",
    "    obs, _, _, _ = vec_env.step(action)\n",
    "\n",
    "# Save the video\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ba28a-e46b-4f99-8967-24b42e55fc97",
   "metadata": {
    "collapsed": false,
    "il": null
   },
   "source": [
    "\n",
    "<a id=\"org1d48a23\"></a>\n",
    "\n",
    "# Execution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdac4f6-fda7-4b33-9c93-41df6df3b25d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To run the script:\n",
    "\n",
    "1.  Ensure that all required libraries are installed.\n",
    "2.  Execute the script by running the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92a09a8-fd9c-4c11-8836-1dd6f66a3709",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "python ddpg-video.py"
   ]
  }
 ],
 "metadata": {
  "il": null,
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "ddpg.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
